{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a090c45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import pandas as pd          # For data manipulation and analysis\n",
    "import numpy as np           # For numerical operations\n",
    "import matplotlib.pyplot as plt  # For creating visualizations\n",
    "import seaborn as sns        # For statistical visualizations (enhances matplotlib)\n",
    "import os                    # For file and directory operations\n",
    "import warnings              # To suppress warning messages\n",
    "from datetime import datetime  # For date and time operations\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fd5b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data loaded successfully!\n",
      "üìä Rows: 3,500\n",
      "üìä Columns: 7\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "os.chdir(\"e:/job/ecommerce_data_project\")\n",
    "df = pd.read_csv(\"data/raw/ecommerce_sales_data.csv\")\n",
    "\n",
    "print(\"‚úÖ Data loaded successfully!\")\n",
    "print(f\"üìä Rows: {df.shape[0]:,}\")\n",
    "print(f\"üìä Columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "919d1613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç INITIAL EXPLORATION\n",
      "==================================================\n",
      "üìã Column information:\n",
      " 1. Order Date                | object     | Null:    0 | Unique: 1051\n",
      " 2. Product Name              | object     | Null:    0 | Unique: 10\n",
      " 3. Category                  | object     | Null:    0 | Unique: 3\n",
      " 4. Region                    | object     | Null:    0 | Unique: 4\n",
      " 5. Quantity                  | int64      | Null:    0 | Unique: 9\n",
      " 6. Sales                     | int64      | Null:    0 | Unique: 2377\n",
      " 7. Profit                    | float64    | Null:    0 | Unique: 3447\n",
      "\n",
      "üßÆ Summary statistics:\n",
      "          Quantity         Sales       Profit\n",
      "count  3500.000000   3500.000000  3500.000000\n",
      "mean      4.931714   3047.966000   527.047203\n",
      "std       2.575895   2440.213237   504.139732\n",
      "min       1.000000     51.000000     6.970000\n",
      "25%       3.000000   1049.500000   158.695000\n",
      "50%       5.000000   2350.500000   361.070000\n",
      "75%       7.000000   4537.000000   729.125000\n",
      "max       9.000000  10782.000000  2946.930000\n",
      "\n",
      "üéØ Data types count:\n",
      "object     4\n",
      "int64      2\n",
      "float64    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# EXPLORE DATA\n",
    "\n",
    "print(\"üîç INITIAL EXPLORATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"üìã Column information:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    dtype = df[col].dtype\n",
    "    null_count = df[col].isnull().sum()\n",
    "    unique_count = df[col].nunique()\n",
    "    print(f\"{i:2}. {col:25} | {str(dtype):10} | Null: {null_count:4} | Unique: {unique_count}\")\n",
    "\n",
    "print(\"\\nüßÆ Summary statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nüéØ Data types count:\")\n",
    "print(df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "972ca72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è MISSING VALUES ANALYSIS\n",
      "==================================================\n",
      "‚úÖ No missing values found!\n"
     ]
    }
   ],
   "source": [
    "# MISSING VALUES\n",
    "\n",
    "print(\"‚ö†Ô∏è MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate missing values\n",
    "missing = df.isnull().sum()\n",
    "missing_percent = (missing / len(df)) * 100\n",
    "\n",
    "# Create DataFrame for better display\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing,\n",
    "    'Missing_Percent': missing_percent.round(2)\n",
    "})\n",
    "\n",
    "# Show only columns with missing values\n",
    "if missing.sum() > 0:\n",
    "    print(\"Columns with missing values:\")\n",
    "    print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    bars = plt.barh(missing_df[missing_df['Missing_Count'] > 0].index,\n",
    "                    missing_df[missing_df['Missing_Count'] > 0]['Missing_Percent'],\n",
    "                    color='salmon')\n",
    "    plt.xlabel('Missing Values (%)')\n",
    "    plt.title('Missing Data by Column')\n",
    "    plt.xlim(0, 100)\n",
    "    \n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(width + 1, bar.get_y() + bar.get_height()/2,\n",
    "                f'{width:.1f}%', ha='left', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚úÖ No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63e46c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ HANDLING MISSING VALUES\n",
      "==================================================\n",
      "‚úÖ Fixed missing values in 0 columns\n",
      "\n",
      "Cleaning actions:\n",
      "\n",
      "üìä Missing values after cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "# HANDLE MISSING VALUES\n",
    "print(\"üßπ HANDLING MISSING VALUES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create a copy for cleaning\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Track cleaning actions\n",
    "cleaning_log = []\n",
    "\n",
    "# Strategy 1: Numeric columns ‚Üí fill with median\n",
    "numeric_cols = df_clean.select_dtypes(include=['int64', 'float64']).columns\n",
    "for col in numeric_cols:\n",
    "    if df_clean[col].isnull().any():\n",
    "        median_val = df_clean[col].median()\n",
    "        df_clean[col].fillna(median_val, inplace=True)\n",
    "        null_count = df[col].isnull().sum()\n",
    "        cleaning_log.append(f\"Filled {null_count} missing values in '{col}' with median: {median_val:.2f}\")\n",
    "\n",
    "# Strategy 2: Text columns ‚Üí fill with mode or 'Unknown'\n",
    "text_cols = df_clean.select_dtypes(include=['object']).columns\n",
    "for col in text_cols:\n",
    "    if df_clean[col].isnull().any():\n",
    "        null_count = df[col].isnull().sum()\n",
    "        if null_count < len(df) * 0.3:  # Less than 30% missing\n",
    "            mode_val = df_clean[col].mode()[0] if not df_clean[col].mode().empty else 'Unknown'\n",
    "            df_clean[col].fillna(mode_val, inplace=True)\n",
    "            cleaning_log.append(f\"Filled {null_count} missing values in '{col}' with mode: '{mode_val}'\")\n",
    "        else:\n",
    "            df_clean[col].fillna('Unknown', inplace=True)\n",
    "            cleaning_log.append(f\"Filled {null_count} missing values in '{col}' with 'Unknown'\")\n",
    "\n",
    "print(f\"‚úÖ Fixed missing values in {len(cleaning_log)} columns\")\n",
    "print(\"\\nCleaning actions:\")\n",
    "for action in cleaning_log:\n",
    "    print(f\"  ‚Ä¢ {action}\")\n",
    "\n",
    "print(f\"\\nüìä Missing values after cleaning: {df_clean.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cdd67ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç REMOVING DUPLICATES\n",
      "==================================================\n",
      "‚úÖ No duplicates found\n"
     ]
    }
   ],
   "source": [
    "# REMOVE DUPLICATES\n",
    "\n",
    "print(\"üîç REMOVING DUPLICATES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "initial_rows = len(df_clean)\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "final_rows = len(df_clean)\n",
    "duplicates_removed = initial_rows - final_rows\n",
    "\n",
    "if duplicates_removed > 0:\n",
    "    cleaning_log.append(f\"Removed {duplicates_removed} duplicate rows\")\n",
    "    print(f\"‚úÖ Removed {duplicates_removed} duplicate rows\")\n",
    "    print(f\"Rows before: {initial_rows:,}\")\n",
    "    print(f\"Rows after: {final_rows:,}\")\n",
    "else:\n",
    "    print(\"‚úÖ No duplicates found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1c3d7637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ SAVING CLEANED DATA\n",
      "==================================================\n",
      "üìç Project root: e:/job/ecommerce_data_project\n",
      "\n",
      "üìÅ Saving to:\n",
      "1. e:/job/ecommerce_data_project\\data\\processed\\cleaned_ecommerce_data.csv\n",
      "2. e:/job/ecommerce_data_project\\data\\processed\\sample_data.csv\n",
      "3. e:/job/ecommerce_data_project\\outputs\\cleaning_log.txt\n",
      "‚úÖ Cleaned data saved: 3,500 rows, 7 columns\n",
      "‚úÖ Sample data saved: 100 rows\n",
      "‚úÖ Cleaning log saved\n",
      "\n",
      "üîç Verification:\n",
      "‚úÖ Clean data: e:/job/ecommerce_data_project\\data\\processed\\cleaned_ecommerce_data.csv (174.9 KB)\n",
      "‚úÖ Sample: e:/job/ecommerce_data_project\\data\\processed\\sample_data.csv (5.0 KB)\n",
      "‚úÖ Log: e:/job/ecommerce_data_project\\outputs\\cleaning_log.txt (0.5 KB)\n",
      "\n",
      "üìÅ Files saved to: e:/job/ecommerce_data_project\n"
     ]
    }
   ],
   "source": [
    "# CELL: SAVE CLEANED DATA (USING ABSOLUTE PATH)\n",
    "print(\"üíæ SAVING CLEANED DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "import os\n",
    "\n",
    "# USE ABSOLUTE PATH - NO GUESSING\n",
    "project_root = \"e:/job/ecommerce_data_project\"\n",
    "print(f\"üìç Project root: {project_root}\")\n",
    "\n",
    "# Define paths INSIDE project folder\n",
    "clean_path = os.path.join(project_root, \"data\", \"processed\", \"cleaned_ecommerce_data.csv\")\n",
    "sample_path = os.path.join(project_root, \"data\", \"processed\", \"sample_data.csv\")\n",
    "log_path = os.path.join(project_root, \"outputs\", \"cleaning_log.txt\")\n",
    "\n",
    "print(f\"\\nüìÅ Saving to:\")\n",
    "print(f\"1. {clean_path}\")\n",
    "print(f\"2. {sample_path}\")\n",
    "print(f\"3. {log_path}\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(os.path.dirname(clean_path), exist_ok=True)  # Creates data/processed\n",
    "os.makedirs(os.path.dirname(log_path), exist_ok=True)    # Creates outputs/\n",
    "\n",
    "# Save the cleaned data\n",
    "df_clean.to_csv(clean_path, index=False)\n",
    "print(f\"‚úÖ Cleaned data saved: {df_clean.shape[0]:,} rows, {df_clean.shape[1]} columns\")\n",
    "\n",
    "# Save a sample\n",
    "df_clean.head(100).to_csv(sample_path, index=False)\n",
    "print(f\"‚úÖ Sample data saved: 100 rows\")\n",
    "\n",
    "# Save cleaning log\n",
    "with open(log_path, 'w') as f:\n",
    "    f.write(\"E-COMMERCE DATA CLEANING LOG\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\\n\")\n",
    "    f.write(f\"Project: E-commerce Sales Data Analysis\\n\")\n",
    "    f.write(f\"Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Original data: {df.shape[0]:,} rows √ó {df.shape[1]} columns\\n\")\n",
    "    f.write(f\"Cleaned data: {df_clean.shape[0]:,} rows √ó {df_clean.shape[1]} columns\\n\\n\")\n",
    "    \n",
    "    f.write(\"DATA CLEANING ACTIONS:\\n\")\n",
    "    f.write(\"-\"*40 + \"\\n\")\n",
    "    for i, action in enumerate(cleaning_log, 1):\n",
    "        f.write(f\"{i}. {action}\\n\")\n",
    "    \n",
    "    f.write(\"\\nDATA QUALITY SUMMARY:\\n\")\n",
    "    f.write(\"-\"*40 + \"\\n\")\n",
    "    f.write(f\"Missing values fixed: {df.isnull().sum().sum() - df_clean.isnull().sum().sum()}\\n\")\n",
    "    f.write(f\"Duplicates removed: {len(df) - len(df_clean)}\\n\")\n",
    "    f.write(f\"New features created: {len(df_clean.columns) - len(df.columns)}\\n\")\n",
    "\n",
    "print(f\"‚úÖ Cleaning log saved\")\n",
    "\n",
    "# Verify files were created\n",
    "print(f\"\\nüîç Verification:\")\n",
    "for path, name in [(clean_path, \"Clean data\"), (sample_path, \"Sample\"), (log_path, \"Log\")]:\n",
    "    if os.path.exists(path):\n",
    "        size_kb = os.path.getsize(path) / 1024\n",
    "        print(f\"‚úÖ {name}: {path} ({size_kb:.1f} KB)\")\n",
    "    else:\n",
    "        print(f\"‚ùå {name} not found: {path}\")\n",
    "\n",
    "print(f\"\\nüìÅ Files saved to: {project_root}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
