{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a090c45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import pandas as pd          # For data manipulation and analysis\n",
    "import numpy as np           # For numerical operations\n",
    "import matplotlib.pyplot as plt  # For creating visualizations\n",
    "import seaborn as sns        # For statistical visualizations (enhances matplotlib)\n",
    "import os                    # For file and directory operations\n",
    "import warnings              # To suppress warning messages\n",
    "from datetime import datetime  # For date and time operations\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fd5b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data loaded successfully!\n",
      "ðŸ“Š Rows: 3,500\n",
      "ðŸ“Š Columns: 7\n"
     ]
    }
   ],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "os.chdir(\"e:/job/ecommerce_data_project\")\n",
    "df = pd.read_csv(\"data/raw/ecommerce_sales_data.csv\")\n",
    "\n",
    "print(\"âœ… Data loaded successfully!\")\n",
    "print(f\"ðŸ“Š Rows: {df.shape[0]:,}\")\n",
    "print(f\"ðŸ“Š Columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "919d1613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” INITIAL EXPLORATION\n",
      "==================================================\n",
      "ðŸ“‹ Column information:\n",
      " 1. Order Date                | object     | Null:    0 | Unique: 1051\n",
      " 2. Product Name              | object     | Null:    0 | Unique: 10\n",
      " 3. Category                  | object     | Null:    0 | Unique: 3\n",
      " 4. Region                    | object     | Null:    0 | Unique: 4\n",
      " 5. Quantity                  | int64      | Null:    0 | Unique: 9\n",
      " 6. Sales                     | int64      | Null:    0 | Unique: 2377\n",
      " 7. Profit                    | float64    | Null:    0 | Unique: 3447\n",
      "\n",
      "ðŸ§® Summary statistics:\n",
      "          Quantity         Sales       Profit\n",
      "count  3500.000000   3500.000000  3500.000000\n",
      "mean      4.931714   3047.966000   527.047203\n",
      "std       2.575895   2440.213237   504.139732\n",
      "min       1.000000     51.000000     6.970000\n",
      "25%       3.000000   1049.500000   158.695000\n",
      "50%       5.000000   2350.500000   361.070000\n",
      "75%       7.000000   4537.000000   729.125000\n",
      "max       9.000000  10782.000000  2946.930000\n",
      "\n",
      "ðŸŽ¯ Data types count:\n",
      "object     4\n",
      "int64      2\n",
      "float64    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# EXPLORE DATA\n",
    "\n",
    "print(\"ðŸ” INITIAL EXPLORATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"ðŸ“‹ Column information:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    dtype = df[col].dtype\n",
    "    null_count = df[col].isnull().sum()\n",
    "    unique_count = df[col].nunique()\n",
    "    print(f\"{i:2}. {col:25} | {str(dtype):10} | Null: {null_count:4} | Unique: {unique_count}\")\n",
    "\n",
    "print(\"\\nðŸ§® Summary statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\nðŸŽ¯ Data types count:\")\n",
    "print(df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "972ca72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ MISSING VALUES ANALYSIS\n",
      "==================================================\n",
      "âœ… No missing values found!\n"
     ]
    }
   ],
   "source": [
    "# MISSING VALUES\n",
    "\n",
    "print(\"âš ï¸ MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate missing values\n",
    "missing = df.isnull().sum()\n",
    "missing_percent = (missing / len(df)) * 100\n",
    "\n",
    "# Create DataFrame for better display\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing,\n",
    "    'Missing_Percent': missing_percent.round(2)\n",
    "})\n",
    "\n",
    "# Show only columns with missing values\n",
    "if missing.sum() > 0:\n",
    "    print(\"Columns with missing values:\")\n",
    "    print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    bars = plt.barh(missing_df[missing_df['Missing_Count'] > 0].index,\n",
    "                    missing_df[missing_df['Missing_Count'] > 0]['Missing_Percent'],\n",
    "                    color='salmon')\n",
    "    plt.xlabel('Missing Values (%)')\n",
    "    plt.title('Missing Data by Column')\n",
    "    plt.xlim(0, 100)\n",
    "    \n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(width + 1, bar.get_y() + bar.get_height()/2,\n",
    "                f'{width:.1f}%', ha='left', va='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âœ… No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63e46c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¹ HANDLING MISSING VALUES\n",
      "==================================================\n",
      "âœ… Fixed missing values in 0 columns\n",
      "\n",
      "Cleaning actions:\n",
      "\n",
      "ðŸ“Š Missing values after cleaning: 0\n"
     ]
    }
   ],
   "source": [
    "# HANDLE MISSING VALUES\n",
    "print(\"ðŸ§¹ HANDLING MISSING VALUES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create a copy for cleaning\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Track cleaning actions\n",
    "cleaning_log = []\n",
    "\n",
    "# Strategy 1: Numeric columns â†’ fill with median\n",
    "numeric_cols = df_clean.select_dtypes(include=['int64', 'float64']).columns\n",
    "for col in numeric_cols:\n",
    "    if df_clean[col].isnull().any():\n",
    "        median_val = df_clean[col].median()\n",
    "        df_clean[col].fillna(median_val, inplace=True)\n",
    "        null_count = df[col].isnull().sum()\n",
    "        cleaning_log.append(f\"Filled {null_count} missing values in '{col}' with median: {median_val:.2f}\")\n",
    "\n",
    "# Strategy 2: Text columns â†’ fill with mode or 'Unknown'\n",
    "text_cols = df_clean.select_dtypes(include=['object']).columns\n",
    "for col in text_cols:\n",
    "    if df_clean[col].isnull().any():\n",
    "        null_count = df[col].isnull().sum()\n",
    "        if null_count < len(df) * 0.3:  # Less than 30% missing\n",
    "            mode_val = df_clean[col].mode()[0] if not df_clean[col].mode().empty else 'Unknown'\n",
    "            df_clean[col].fillna(mode_val, inplace=True)\n",
    "            cleaning_log.append(f\"Filled {null_count} missing values in '{col}' with mode: '{mode_val}'\")\n",
    "        else:\n",
    "            df_clean[col].fillna('Unknown', inplace=True)\n",
    "            cleaning_log.append(f\"Filled {null_count} missing values in '{col}' with 'Unknown'\")\n",
    "\n",
    "print(f\"âœ… Fixed missing values in {len(cleaning_log)} columns\")\n",
    "print(\"\\nCleaning actions:\")\n",
    "for action in cleaning_log:\n",
    "    print(f\"  â€¢ {action}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Missing values after cleaning: {df_clean.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cdd67ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” REMOVING DUPLICATES\n",
      "==================================================\n",
      "âœ… No duplicates found\n"
     ]
    }
   ],
   "source": [
    "# REMOVE DUPLICATES\n",
    "\n",
    "print(\"ðŸ” REMOVING DUPLICATES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "initial_rows = len(df_clean)\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "final_rows = len(df_clean)\n",
    "duplicates_removed = initial_rows - final_rows\n",
    "\n",
    "if duplicates_removed > 0:\n",
    "    cleaning_log.append(f\"Removed {duplicates_removed} duplicate rows\")\n",
    "    print(f\"âœ… Removed {duplicates_removed} duplicate rows\")\n",
    "    print(f\"Rows before: {initial_rows:,}\")\n",
    "    print(f\"Rows after: {final_rows:,}\")\n",
    "else:\n",
    "    print(\"âœ… No duplicates found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1c3d7637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ SAVING CLEANED DATA\n",
      "==================================================\n",
      "ðŸ“ Project root: e:/job/ecommerce_data_project\n",
      "\n",
      "ðŸ“ Saving to:\n",
      "1. e:/job/ecommerce_data_project\\data\\processed\\cleaned_ecommerce_data.csv\n",
      "2. e:/job/ecommerce_data_project\\data\\processed\\sample_data.csv\n",
      "3. e:/job/ecommerce_data_project\\outputs\\cleaning_log.txt\n",
      "âœ… Cleaned data saved: 3,500 rows, 7 columns\n",
      "âœ… Sample data saved: 100 rows\n",
      "âœ… Cleaning log saved\n",
      "\n",
      "ðŸ” Verification:\n",
      "âœ… Clean data: e:/job/ecommerce_data_project\\data\\processed\\cleaned_ecommerce_data.csv (174.9 KB)\n",
      "âœ… Sample: e:/job/ecommerce_data_project\\data\\processed\\sample_data.csv (5.0 KB)\n",
      "âœ… Log: e:/job/ecommerce_data_project\\outputs\\cleaning_log.txt (0.5 KB)\n",
      "\n",
      "ðŸ“ Files saved to: e:/job/ecommerce_data_project\n"
     ]
    }
   ],
   "source": [
    "# CELL: SAVE CLEANED DATA (USING ABSOLUTE PATH)\n",
    "print(\"ðŸ’¾ SAVING CLEANED DATA\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "import os\n",
    "\n",
    "# USE ABSOLUTE PATH - NO GUESSING\n",
    "project_root = \"e:/job/ecommerce_data_project\"\n",
    "print(f\"ðŸ“ Project root: {project_root}\")\n",
    "\n",
    "# Define paths INSIDE project folder\n",
    "clean_path = os.path.join(project_root, \"data\", \"processed\", \"cleaned_ecommerce_data.csv\")\n",
    "sample_path = os.path.join(project_root, \"data\", \"processed\", \"sample_data.csv\")\n",
    "log_path = os.path.join(project_root, \"outputs\", \"cleaning_log.txt\")\n",
    "\n",
    "print(f\"\\nðŸ“ Saving to:\")\n",
    "print(f\"1. {clean_path}\")\n",
    "print(f\"2. {sample_path}\")\n",
    "print(f\"3. {log_path}\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(os.path.dirname(clean_path), exist_ok=True)  # Creates data/processed\n",
    "os.makedirs(os.path.dirname(log_path), exist_ok=True)    # Creates outputs/\n",
    "\n",
    "# Save the cleaned data\n",
    "df_clean.to_csv(clean_path, index=False)\n",
    "print(f\"âœ… Cleaned data saved: {df_clean.shape[0]:,} rows, {df_clean.shape[1]} columns\")\n",
    "\n",
    "# Save a sample\n",
    "df_clean.head(100).to_csv(sample_path, index=False)\n",
    "print(f\"âœ… Sample data saved: 100 rows\")\n",
    "\n",
    "# Save cleaning log\n",
    "with open(log_path, 'w') as f:\n",
    "    f.write(\"E-COMMERCE DATA CLEANING LOG\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\\n\")\n",
    "    f.write(f\"Project: E-commerce Sales Data Analysis\\n\")\n",
    "    f.write(f\"Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Original data: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\\n\")\n",
    "    f.write(f\"Cleaned data: {df_clean.shape[0]:,} rows Ã— {df_clean.shape[1]} columns\\n\\n\")\n",
    "    \n",
    "    f.write(\"DATA CLEANING ACTIONS:\\n\")\n",
    "    f.write(\"-\"*40 + \"\\n\")\n",
    "    for i, action in enumerate(cleaning_log, 1):\n",
    "        f.write(f\"{i}. {action}\\n\")\n",
    "    \n",
    "    f.write(\"\\nDATA QUALITY SUMMARY:\\n\")\n",
    "    f.write(\"-\"*40 + \"\\n\")\n",
    "    f.write(f\"Missing values fixed: {df.isnull().sum().sum() - df_clean.isnull().sum().sum()}\\n\")\n",
    "    f.write(f\"Duplicates removed: {len(df) - len(df_clean)}\\n\")\n",
    "    f.write(f\"New features created: {len(df_clean.columns) - len(df.columns)}\\n\")\n",
    "\n",
    "print(f\"âœ… Cleaning log saved\")\n",
    "\n",
    "# Verify files were created\n",
    "print(f\"\\nðŸ” Verification:\")\n",
    "for path, name in [(clean_path, \"Clean data\"), (sample_path, \"Sample\"), (log_path, \"Log\")]:\n",
    "    if os.path.exists(path):\n",
    "        size_kb = os.path.getsize(path) / 1024\n",
    "        print(f\"âœ… {name}: {path} ({size_kb:.1f} KB)\")\n",
    "    else:\n",
    "        print(f\"âŒ {name} not found: {path}\")\n",
    "\n",
    "print(f\"\\nðŸ“ Files saved to: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "947978bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ CHECKING PROJECT STATUS\n",
      "============================================================\n",
      "Project location: e:/job/ecommerce_data_project\n",
      "\n",
      "ðŸ“Š KEY FILES TO COMMIT:\n",
      "\n",
      "ðŸ““ Notebooks:\n",
      "  âœ… 01_data_cleaning.ipynb.ipynb (13.3 KB)\n",
      "\n",
      "ðŸ“Š Data files (processed):\n",
      "  âœ… cleaned_ecommerce_data.csv (174.9 KB)\n",
      "  âœ… sample_data.csv (5.0 KB)\n",
      "\n",
      "ðŸ“ Output files:\n",
      "  âœ… cleaning_log.txt (0.5 KB)\n",
      "  âœ… plots (0.0 KB)\n",
      "  âœ… reports (0.0 KB)\n",
      "\n",
      "ðŸ“„ Project configuration files:\n",
      "  âœ… .gitignore (0.4 KB)\n",
      "  âœ… README.md (2.1 KB)\n",
      "  âœ… requirements.txt (31.7 KB)\n",
      "  âš ï¸  LICENSE (missing)\n",
      "  âš ï¸  project_info.json (missing)\n",
      "\n",
      "============================================================\n",
      "âœ… Ready for GitHub push!\n"
     ]
    }
   ],
   "source": [
    "# CELL: CHECK PROJECT STATUS\n",
    "print(\"ðŸ“ CHECKING PROJECT STATUS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Project root\n",
    "project_root = \"e:/job/ecommerce_data_project\"\n",
    "print(f\"Project location: {project_root}\")\n",
    "\n",
    "# List all important files\n",
    "print(\"\\nðŸ“Š KEY FILES TO COMMIT:\")\n",
    "\n",
    "# 1. Notebooks\n",
    "print(\"\\nðŸ““ Notebooks:\")\n",
    "notebooks_path = os.path.join(project_root, \"notebooks\")\n",
    "if os.path.exists(notebooks_path):\n",
    "    for file in os.listdir(notebooks_path):\n",
    "        if file.endswith('.ipynb'):\n",
    "            size = os.path.getsize(os.path.join(notebooks_path, file)) / 1024\n",
    "            print(f\"  âœ… {file} ({size:.1f} KB)\")\n",
    "\n",
    "# 2. Data files (processed only - raw should be in .gitignore)\n",
    "print(\"\\nðŸ“Š Data files (processed):\")\n",
    "processed_path = os.path.join(project_root, \"data\", \"processed\")\n",
    "if os.path.exists(processed_path):\n",
    "    for file in os.listdir(processed_path):\n",
    "        if file.endswith('.csv'):\n",
    "            size = os.path.getsize(os.path.join(processed_path, file)) / 1024\n",
    "            print(f\"  âœ… {file} ({size:.1f} KB)\")\n",
    "\n",
    "# 3. Outputs\n",
    "print(\"\\nðŸ“ Output files:\")\n",
    "outputs_path = os.path.join(project_root, \"outputs\")\n",
    "if os.path.exists(outputs_path):\n",
    "    for file in os.listdir(outputs_path):\n",
    "        size = os.path.getsize(os.path.join(outputs_path, file)) / 1024\n",
    "        print(f\"  âœ… {file} ({size:.1f} KB)\")\n",
    "\n",
    "# 4. Project files\n",
    "print(\"\\nðŸ“„ Project configuration files:\")\n",
    "root_files = ['.gitignore', 'README.md', 'requirements.txt', 'LICENSE', 'project_info.json']\n",
    "for file in root_files:\n",
    "    path = os.path.join(project_root, file)\n",
    "    if os.path.exists(path):\n",
    "        size = os.path.getsize(path) / 1024\n",
    "        print(f\"  âœ… {file} ({size:.1f} KB)\")\n",
    "    else:\n",
    "        print(f\"  âš ï¸  {file} (missing)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Ready for GitHub push!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "04332f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ CREATING MISSING PROJECT FILES\n",
      "============================================================\n",
      "âœ… Created: project_info.json\n",
      "\n",
      "âœ… All project files ready!\n"
     ]
    }
   ],
   "source": [
    "# CELL: CREATE MISSING FILES\n",
    "print(\"ðŸ“ CREATING MISSING PROJECT FILES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "project_root = \"e:/job/ecommerce_data_project\"\n",
    "\n",
    "# 1. Create requirements.txt if missing\n",
    "req_path = os.path.join(project_root, \"requirements.txt\")\n",
    "if not os.path.exists(req_path):\n",
    "    requirements = \"\"\"pandas>=1.5.0\n",
    "numpy>=1.21.0\n",
    "matplotlib>=3.5.0\n",
    "seaborn>=0.11.0\n",
    "jupyter>=1.0.0\n",
    "\"\"\"\n",
    "    with open(req_path, 'w') as f:\n",
    "        f.write(requirements)\n",
    "    print(\"âœ… Created: requirements.txt\")\n",
    "\n",
    "# 2. Create .gitignore if missing\n",
    "gitignore_path = os.path.join(project_root, \".gitignore\")\n",
    "if not os.path.exists(gitignore_path):\n",
    "    gitignore_content = \"\"\"# Python\n",
    "__pycache__/\n",
    "*.py[cod]\n",
    "*$py.class\n",
    "*.so\n",
    ".Python\n",
    "env/\n",
    "venv/\n",
    ".venv/\n",
    "ENV/\n",
    "env.bak/\n",
    "venv.bak/\n",
    "\n",
    "# Jupyter Notebook\n",
    ".ipynb_checkpoints/\n",
    "*.ipynb_checkpoints/\n",
    "\n",
    "# IDE\n",
    ".vscode/\n",
    ".idea/\n",
    "*.swp\n",
    "*.swo\n",
    "\n",
    "# OS\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "\n",
    "# Data files (large files - don't upload to GitHub)\n",
    "data/raw/\n",
    "*.csv\n",
    "*.xlsx\n",
    "*.pkl\n",
    "*.h5\n",
    "*.feather\n",
    "*.parquet\n",
    "\n",
    "# Outputs (can be regenerated)\n",
    "outputs/\n",
    "\n",
    "# Git\n",
    ".git/\n",
    "\"\"\"\n",
    "    with open(gitignore_path, 'w') as f:\n",
    "        f.write(gitignore_content)\n",
    "    print(\"âœ… Created: .gitignore\")\n",
    "\n",
    "# 3. Create project_info.json if missing\n",
    "info_path = os.path.join(project_root, \"project_info.json\")\n",
    "if not os.path.exists(info_path):\n",
    "    project_info = {\n",
    "        \"project_name\": \"E-commerce Data Cleaning Portfolio Project\",\n",
    "        \"author\": \"snow6060\",\n",
    "        \"description\": \"Data cleaning pipeline demonstrating Python/Pandas skills for data engineering roles\",\n",
    "        \"technologies\": [\"Python\", \"Pandas\", \"NumPy\", \"Matplotlib\", \"Seaborn\", \"Jupyter\"],\n",
    "        \"dataset\": \"E-commerce sales data (2022-2024)\",\n",
    "        \"github\": \"https://github.com/snow6060/ecommerce-data-project\",\n",
    "        \"created\": \"2024\",\n",
    "        \"last_updated\": \"2024\"\n",
    "    }\n",
    "    with open(info_path, 'w') as f:\n",
    "        json.dump(project_info, f, indent=2)\n",
    "    print(\"âœ… Created: project_info.json\")\n",
    "\n",
    "print(\"\\nâœ… All project files ready!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
